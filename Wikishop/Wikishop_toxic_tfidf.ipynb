{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe878d0",
   "metadata": {},
   "source": [
    "# Проект для \"Викишоп\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212dccfe",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.  \n",
    "\n",
    "Задача - обучить модель классифицировать комментарии на позитивные и негативные. Для обучения предоставлен набор данных с разметкой о токсичности правок. Метрика качества модели F1 должна быть не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ae172",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "\n",
    "* [1. Подготовка данных](#chapter1)\n",
    "* [2. Обучение моделей](#chapter2)\n",
    "* [3. Вывод](#chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a496db",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755f59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9606bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import spacy\n",
    "from spacy import load\n",
    "from spacy.lang.en.examples import sentences\n",
    "from spacy.lang.en import English\n",
    "import lightgbm as lgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2ffb50-dadc-49ae-a717-f6696f74f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b2e710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e18a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94507e25",
   "metadata": {},
   "source": [
    "Столбец *Unnamed: 0* дублирует номера строк, удалим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d3d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97750f",
   "metadata": {},
   "source": [
    "Проверим данные на наличие дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507cbcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61837dbe",
   "metadata": {},
   "source": [
    "Дубликатов нет.  \n",
    "Проверим данные на дисбаланс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e21917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего записей 159292, из них токсичных 16186\n"
     ]
    }
   ],
   "source": [
    "toxic = data['toxic'].sum()\n",
    "print(f'Всего записей {data.shape[0]}, из них токсичных {toxic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fa2b4",
   "metadata": {},
   "source": [
    "Присутствует дисбаланс классов. В дальнейшем стоит обратить на это внимание, если не получится добиться желаемых результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bb45e",
   "metadata": {},
   "source": [
    "Данные загружены.  \n",
    "Подготовим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d84b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#небольшая выборка для отладки\n",
    "#data = data.sample(40000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0498631",
   "metadata": {},
   "source": [
    "Очистим данные:  \n",
    "* Избавимся от знаков препинания и чисел, заменив их на пробелы  \n",
    "* Уберем двойные пробелы  \n",
    "* Приведем текст к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157befe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation why the edits made under my userna...\n",
       "1         d aww he matches this background colour i m se...\n",
       "2         hey man i m really not trying to edit war it s...\n",
       "3          more i can t make any real suggestions on imp...\n",
       "4         you sir are my hero any chance you remember wh...\n",
       "                                ...                        \n",
       "159287     and for the second time of asking when your v...\n",
       "159288    you should be ashamed of yourself that is a ho...\n",
       "159289    spitzer umm theres no actual article for prost...\n",
       "159290    and it looks like it was actually you who put ...\n",
       "159291     and i really don t think you understand i cam...\n",
       "Name: text_clean, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_clean'] = data['text'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()\n",
    "data['text_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f6e74",
   "metadata": {},
   "source": [
    "Лемматизируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77dd08eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>[explanation, why, the, edit, make, under, my,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>[d, aww, he, match, this, background, colour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>[hey, man, I, m, really, not, try, to, edit, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on imp...</td>\n",
       "      <td>[ , more, I, can, t, make, any, real, suggesti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>[you, sir, be, my, hero, any, chance, you, rem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d aww he matches this background colour i m se...   \n",
       "2  hey man i m really not trying to edit war it s...   \n",
       "3   more i can t make any real suggestions on imp...   \n",
       "4  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                    text_clean_lemma  \n",
       "0  [explanation, why, the, edit, make, under, my,...  \n",
       "1  [d, aww, he, match, this, background, colour, ...  \n",
       "2  [hey, man, I, m, really, not, try, to, edit, w...  \n",
       "3  [ , more, I, can, t, make, any, real, suggesti...  \n",
       "4  [you, sir, be, my, hero, any, chance, you, rem...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemma = []\n",
    " \n",
    "for doc in nlp.pipe(data[\"text_clean\"].values):\n",
    "    lemma.append([n.lemma_ for n in doc])\n",
    "\n",
    "data['text_clean_lemma'] = lemma\n",
    "data[['text_clean','text_clean_lemma']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db2abf",
   "metadata": {},
   "source": [
    "Уберем стоп-слова и \"склеим\" лемматизированные слова в строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2905b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33243935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation edit make username hardcore metall...\n",
       "1         aww match background colour I seemingly stuck ...\n",
       "2         hey man I really try edit war guy constantly r...\n",
       "3           I make real suggestion improvement I wonder ...\n",
       "4                             sir hero chance remember page\n",
       "                                ...                        \n",
       "159287      second time ask view completely contradict c...\n",
       "159288    ashamed horrible thing put talk page 128 61 19 93\n",
       "159289    spitzer umm actual article prostitution ring c...\n",
       "159290    look like actually put speedy first version de...\n",
       "159291      I really think understand I come idea bad ri...\n",
       "Name: text_clean_lemma_as_str, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_clean_lemma'] = data['text_clean_lemma'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "data['text_clean_lemma_as_str'] = [' '.join(map(str, l)) for l in data['text_clean_lemma']]\n",
    "data['text_clean_lemma_as_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8222325",
   "metadata": {},
   "source": [
    "Получен массив лемматизированных текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd4bcc",
   "metadata": {},
   "source": [
    "## 2. Обучение моделей<a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef5c1b",
   "metadata": {},
   "source": [
    "Разобьем данные на тренировочную, валидационную и тестовую выборки в отношении 60:20:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efe658d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = pd.DataFrame(features)\n",
    "features_train, features_test, target_train, target_test = train_test_split(data['text_clean_lemma_as_str'], data['toxic'], test_size=0.2)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838bd87",
   "metadata": {},
   "source": [
    "Вычислим TF-IDF для всех выборок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a9cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "features_train = tf_idf.fit_transform(features_train)\n",
    "features_valid = tf_idf.transform(features_valid)\n",
    "features_test = tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7669a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train (95574, 124279), target_train (95574,)\n",
      "features_valid (31859, 124279), target_valid (31859,)\n",
      "features_test (31859, 124279), target_test (31859,)\n"
     ]
    }
   ],
   "source": [
    "print(f'features_train {features_train.shape}, target_train {target_train.shape}')\n",
    "print(f'features_valid {features_valid.shape}, target_valid {target_valid.shape}')\n",
    "print(f'features_test {features_test.shape}, target_test {target_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4d264",
   "metadata": {},
   "source": [
    "Рассмотрим модель несколько моделей и выберем лучшую.  \n",
    "Начнем с логистической регрессии. Т.к. в данных присутствует дисбаланс классов, укажем соответствующий гиперпараметр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03723a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 без учета дисбаланса равна 0.73\n",
      "F1 с учетом дисбаланса равна 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1 = f1_score(target_valid, predicted_valid)\n",
    "print(f'F1 без учета дисбаланса равна {f1:.2f}')\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "f1 = f1_score(target_valid, predicted_valid)\n",
    "print(f'F1 с учетом дисбаланса равна {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c101d",
   "metadata": {},
   "source": [
    "Получили хороший результат, учет дисбаланса положительно сказался на качестве.  \n",
    "Проверим модель градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f94a820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты с учетом дисбаланса\n",
      "Learning_rate 0.1, Num_iterations 100, F1 равна 0.72\n",
      "Learning_rate 0.1, Num_iterations 200, F1 равна 0.73\n",
      "Learning_rate 0.1, Num_iterations 300, F1 равна 0.74\n",
      "Learning_rate 0.5, Num_iterations 100, F1 равна 0.74\n",
      "Learning_rate 0.5, Num_iterations 200, F1 равна 0.75\n",
      "Learning_rate 0.5, Num_iterations 300, F1 равна 0.76\n",
      "Результаты без учета дисбаланса\n",
      "Learning_rate 0.1, Num_iterations 100, F1 равна 0.75\n",
      "Learning_rate 0.1, Num_iterations 200, F1 равна 0.77\n",
      "Learning_rate 0.1, Num_iterations 300, F1 равна 0.77\n",
      "Learning_rate 0.5, Num_iterations 100, F1 равна 0.76\n",
      "Learning_rate 0.5, Num_iterations 200, F1 равна 0.76\n",
      "Learning_rate 0.5, Num_iterations 300, F1 равна 0.76\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "print('Результаты с учетом дисбаланса')\n",
    "\n",
    "for lrate in [0.1, 0.5]:\n",
    "    for num_iters in [100, 200, 300]:\n",
    "        param = {\n",
    "            'objective': 'binary',\n",
    "            'metric':'binary_error',\n",
    "            'is_unbalance':True,\n",
    "            'num_iterations':num_iters, \n",
    "            'learning_rate':lrate, \n",
    "            \"verbose\":-100\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "        model.fit(features_train, target_train)\n",
    "        predicted_valid = model.predict(features_valid)\n",
    "        f1 = f1_score(target_valid, predicted_valid)\n",
    "        print(f\"Learning_rate {lrate}, Num_iterations {num_iters}, F1 равна {f1:.2f}\")\n",
    "\n",
    "print('Результаты без учета дисбаланса')\n",
    "\n",
    "for lrate in [0.1, 0.5]:\n",
    "    for num_iters in [100, 200, 300]:\n",
    "        param = {\n",
    "            'objective': 'binary',\n",
    "            'metric':'binary_error',\n",
    "            'is_unbalance':False,\n",
    "            'num_iterations':num_iters, \n",
    "            'learning_rate':lrate, \n",
    "            \"verbose\":-100\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "        model.fit(features_train, target_train)\n",
    "        predicted_valid = model.predict(features_valid)\n",
    "        f1 = f1_score(target_valid, predicted_valid)\n",
    "        print(f\"Learning_rate {lrate}, Num_iterations {num_iters}, F1 равна {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745285a",
   "metadata": {},
   "source": [
    "Результат аналогичен логистической регрессии.  \n",
    "Возьмем в качестве гиперпараметров значения `Learning_rate` = 0.1, `Num_iterations` = 200.  \n",
    "Дисбаланс классов учитывать не будем, т.к. в этом случае результат модели хуже.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39be577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший результат 0.33, количество деревьев 30, глубина дерева 5\n"
     ]
    }
   ],
   "source": [
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for depth in range(2,7):\n",
    "    for est in range(10, 51, 10):\n",
    "        model = RandomForestClassifier(class_weight='balanced', n_estimators=est, max_depth=depth)\n",
    "        model.fit(features_train, target_train)\n",
    "        predicted_valid = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, predicted_valid)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "        \n",
    "print(f'Лучший результат {best_result:.2f}, количество деревьев {best_est}, глубина дерева {best_depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5b99e",
   "metadata": {},
   "source": [
    "Результат неудовлетворительный."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb08a6",
   "metadata": {},
   "source": [
    "Модель логистической регрессии показала результат не хуже, чем у градиентного бустинга. Остановимся на ней.  \n",
    "Проверим модель логистической регрессии на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfd72988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 с учетом дисбаланса равна 0.75\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_test = model.predict(features_test)\n",
    "f1 = f1_score(target_test, predicted_test)\n",
    "print(f'F1 с учетом дисбаланса равна {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6e24",
   "metadata": {},
   "source": [
    "Проверим модель на адекватность.  \n",
    "Чтобы учесть дисбаланс, выберем стратегию `stratified`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82a8bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 с учетом дисбаланса равна 0.10\n"
     ]
    }
   ],
   "source": [
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_test = model.predict(features_test)\n",
    "f1 = f1_score(target_test, predicted_test)\n",
    "print(f'F1 с учетом дисбаланса равна {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d10014",
   "metadata": {},
   "source": [
    "Метрика ухудшилась, значит наша модель эффективнее случайных \"угадываний\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e84e7",
   "metadata": {},
   "source": [
    "## 3. Вывод<a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5267de",
   "metadata": {},
   "source": [
    "Удалось получить модель с качеством, удовлетворяющим условиям задачи.  \n",
    "Исходные данные были обработаны:\n",
    "- удалены знаки препинания, числа, лишние пробелы,\n",
    "- удалены стоп-слова (предлоги, местоимения, междометия и другие слова, не несущие большой смысловой нагрузки),\n",
    "- тексты были лемматизированы.  \n",
    "\n",
    "Были вычеслены величины TF-IDF для корпуса текстов. Эти величины были использованы в качестве признаков для обучения моделей классификации.  \n",
    "\n",
    "Было рассмотрено несколько моделей с различными гиперпараметрами: логистическая регрессия, градиентный бустинг и случайный лес.  \n",
    "Логистическая регрессия и градиентный бустинг показали похожий результат, но выбор сделан в пользу первого из-за скорости вычислений. Модель случайного леса дала неудовлетворительный результат.  \n",
    "В итоге, модель логистической регрессии подтвердила результат на тестовых данных и прошла проверку на адекватность.  \n",
    "Итоговая метрика качества F1 равна 0.75."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
